#!/bin/bash

#SBATCH -n 6
#SBATCH -N 1
#SBATCH -t 0-1:00
#SBATCH -p huce_intel
#SBATCH --mem=100000
#SBATCH --mail-type=ALL

# See SLURM documentation for descriptions of the above settings as well
# as many other settings you may use.

# Define GEOS-Chem log file
log="gchp.log"

# Disable this if running consecutive jobs with the duration in which case
# cap_restart is used, i.e. start and end span 1-year but duration is 1-month.
if [[ -e cap_restart ]]; then
   rm cap_restart
fi

# Sync all config files with settings in runConfig.sh                           
./runConfig.sh > runConfig.log
if [[ $? == 0 ]]; then

    # Set and source your bashrc. Change this to whatever env file
    # used during GCHP compilation.
    BASHRC=bashrcSamples/gchp.ifort15_openmpi_odyssey.bashrc
    echo "WARNING: You are using environment settings in $BASHRC"
    source $BASHRC

    # Use SLURM to distribute tasks across nodes
    NX=$( grep NX GCHP.rc | awk '{print $2}' )
    NY=$( grep NY GCHP.rc | awk '{print $2}' )
    coreCount=$(( ${NX} * ${NY} ))
    planeCount=$(( ${coreCount} / ${SLURM_NNODES} ))
    if [[ $(( ${coreCount} % ${SLURM_NNODES} )) > 0 ]]; then
	${planeCount}=$(( ${planeCount} + 1 ))
    fi

    # Echo info from computational cores to log file for displaying results
    echo "# of CPUs : $coreCount"
    echo "# of nodes: $SLURM_NNODES"
    echo "-m plane  : ${planeCount}"

    # Echo start date
    echo '===> Run started at' `date` >> ${log}

    # If the shared memory option is on (USE_SHMEM: 1 in CAP.rc) then
    # clean up existing shared memory segments before and after running 
    # the model
    if grep -q "USE_SHMEM.*1" CAP.rc; then
        ipcs -a  # Show current shared memory segments
        ipcs -l  # Show shared memory limits
        /sbin/sysctl -A | grep shm
        memCleaner=./runScriptSamples/rmshmem.sh
        srun -w $SLURM_JOB_NODELIST -n $SLURM_NNODES --ntasks-per-node=1 ${memCleaner}
        ipcs -a  # Show current shared memory segments again (should be none)
    fi

    # Start the simulation
    time srun -n ${coreCount} -N ${SLURM_NNODES} -m plane=${planeCount} --mpi=pmi2 ./geos >> ${log}

    # Clean up any shared memory segments owned by this user
    if grep -q "USE_SHMEM.*1" CAP.rc; then
        srun -w $SLURM_JOB_NODELIST -n $SLURM_NNODES --ntasks-per-node=1 ${memCleaner}
    fi

    # Echo end date
    echo '===> Run ended at' `date` >> ${log}

else
    cat runConfig.log
fi

# Clear variable
unset log

# Exit normally
exit 0

